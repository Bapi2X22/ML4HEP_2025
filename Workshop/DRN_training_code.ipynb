{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df337b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !micromamba activate higgs-dna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50a62cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import awkward as ak\n",
    "import torch\n",
    "import psutil\n",
    "from torch_geometric.data import Data\n",
    "# import h5py\n",
    "# from torch_geometric.loader import DataLoader\n",
    "# import os\n",
    "# import os.path as osp\n",
    "# import math\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import gc\n",
    "# import torch.nn as nn\n",
    "# from torch.nn.functional import softplus\n",
    "# import torch_geometric.transforms as T\n",
    "\n",
    "# from torch.utils.checkpoint import checkpoint\n",
    "# from torch_cluster import knn_graph, graclus_cluster\n",
    "# from torch_scatter import scatter\n",
    "# from torch_sparse.storage import SparseStorage\n",
    "\n",
    "# from torch import Tensor\n",
    "# from torch_geometric.typing import OptTensor, Optional, Tuple\n",
    "\n",
    "\n",
    "# from torch_geometric.nn import EdgeConv, NNConv\n",
    "# from torch_geometric.nn.pool.pool import pool_batch\n",
    "# from torch_geometric.nn.pool.consecutive import consecutive_cluster\n",
    "# from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "# from torch_geometric.utils import normalized_cut\n",
    "# from torch_geometric.utils import remove_self_loops\n",
    "# from torch_geometric.nn import (max_pool, max_pool_x, global_max_pool,\n",
    "#                                 avg_pool, avg_pool_x, global_mean_pool, \n",
    "#                                 global_add_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ded97c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Fri_Feb_21_20:23:50_PST_2025\n",
      "Cuda compilation tools, release 12.8, V12.8.93\n",
      "Build cuda_12.8.r12.8/compiler.35583870_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a35f4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 11 04:53:17 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:09:00.0 Off |                    0 |\n",
      "| N/A   77C    P0             64W /   70W |    6245MiB /  15360MiB |    100%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c74025",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Cartesian(cat=False)\n",
    "\n",
    "def normalized_cut_2d(edge_index, pos):\n",
    "    row, col = edge_index[0], edge_index[1]\n",
    "    edge_attr = torch.norm(pos[row] - pos[col], p=2, dim=1)\n",
    "    return normalized_cut(edge_index, edge_attr, num_nodes=pos.size(0))\n",
    "\n",
    "# jit compatible version of coalesce\n",
    "def coalesce(index, value: OptTensor, m: int, n: int, op: str = \"add\"):\n",
    "    storage = SparseStorage(row=index[0], col=index[1], value=value,\n",
    "                            sparse_sizes=(m, n), is_sorted=False)\n",
    "    storage = storage.coalesce(reduce=op)\n",
    "    return torch.stack([storage.row(), storage.col()], dim=0), storage.value()\n",
    "\n",
    "# jit compatible version of to_undirected\n",
    "def to_undirected(edge_index, num_nodes: Optional[int] = None) -> Tensor:\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    row, col = edge_index[0], edge_index[1]\n",
    "    temp = torch.cat([row, col], dim=0), torch.cat([col, row], dim=0)\n",
    "    row, col = temp[0], temp[1]\n",
    "    edge_index = torch.stack([row, col], dim=0)\n",
    "    edge_index, _ = coalesce(edge_index, None, num_nodes, num_nodes)\n",
    "    return edge_index\n",
    "\n",
    "# jit compatible version of pool_edge, depends on coalesce\n",
    "def pool_edge(cluster, edge_index, edge_attr: Optional[torch.Tensor] = None):\n",
    "    num_nodes = cluster.size(0)\n",
    "    edge_index = cluster[edge_index.view(-1)].view(2, -1)\n",
    "    edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
    "    if edge_index.numel() > 0:\n",
    "        edge_index, edge_attr = coalesce(edge_index, edge_attr, num_nodes,\n",
    "                                         num_nodes)\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "def _aggr_pool_x(cluster, x, aggr: str, size: Optional[int] = None):\n",
    "    \"\"\"Call into scatter with configurable reduction op\"\"\"\n",
    "    return scatter(x, cluster, dim=0, dim_size=size, reduce=aggr)\n",
    "\n",
    "def global_pool_aggr(x, batch: OptTensor, aggr: str, size: Optional[int] = None):\n",
    "    \"\"\"Global pool via passed aggregator: 'mean', 'add', 'max'\"\"\"\n",
    "    if batch is None and size is None:\n",
    "        raise Exception('Must provide at least one of \"batch\" or \"size\"')\n",
    "    if batch is not None:\n",
    "        size = int(batch.max().item() + 1)\n",
    "    assert batch is not None\n",
    "    return scatter(x, batch, dim=0, dim_size=size, reduce=aggr)\n",
    "\n",
    "# this function is specialized compared to the more general non-jittable version\n",
    "# in particular edge_attr can be removed since it is always None\n",
    "def aggr_pool(cluster, x, batch: OptTensor, aggr: str) -> Tuple[Tensor, OptTensor]:\n",
    "    \"\"\"jit-friendly version of max/mean/add pool\"\"\"\n",
    "    cluster, perm = consecutive_cluster(cluster)\n",
    "    x = _aggr_pool_x(cluster, x, aggr)\n",
    "    if batch is not None:\n",
    "        batch = pool_batch(perm, batch)\n",
    "    return x, batch\n",
    "\n",
    "def aggr_pool_x(cluster, x, batch: OptTensor, aggr: str, size: Optional[int] = None) -> Tuple[Tensor, OptTensor]:\n",
    "    \"\"\"*_pool_x with configurable aggr method\"\"\"\n",
    "    if batch is None and size is None:\n",
    "        raise Exception('Must provide at least one of \"batch\" or \"size\"')\n",
    "    if size is not None and batch is not None:\n",
    "        batch_size = int(batch.max().item()) + 1\n",
    "        return _aggr_pool_x(cluster, x, aggr, batch_size * size), None\n",
    "\n",
    "    cluster, perm = consecutive_cluster(cluster)\n",
    "    x = _aggr_pool_x(cluster, x, aggr)\n",
    "    if batch is not None:\n",
    "        batch = pool_batch(perm, batch)\n",
    "\n",
    "    return x, batch\n",
    "    \n",
    "class DynamicReductionNetworkJit(nn.Module):\n",
    "    '''\n",
    "    This model iteratively contracts nearest neighbour graphs \n",
    "    until there is one output node.\n",
    "    The latent space trained to group useful features at each level\n",
    "    of aggregration.\n",
    "    This allows single quantities to be regressed from complex point counts\n",
    "    in a location and orientation invariant way.\n",
    "    One encoding layer is used to abstract away the input features.\n",
    "\n",
    "    @param input_dim: dimension of input features\n",
    "    @param hidden_dim: dimension of hidden layers\n",
    "    @param output_dim: dimensio of output\n",
    "    \n",
    "    @param k: size of k-nearest neighbor graphs\n",
    "    @param aggr: message passing aggregation scheme. \n",
    "    @param norm: feature normaliztion. None is equivalent to all 1s (ie no scaling)\n",
    "    @param loop: boolean for presence/absence of self loops in k-nearest neighbor graphs\n",
    "    @param pool: type of pooling in aggregation layers. Choices are 'add', 'max', 'mean'\n",
    "    \n",
    "    @param agg_layers: number of aggregation layers. Must be >=0\n",
    "    @param mp_layers: number of layers in message passing networks. Must be >=1\n",
    "    @param in_layers: number of layers in inputnet. Must be >=1\n",
    "    @param out_layers: number of layers in outputnet. Must be >=1\n",
    "    '''\n",
    "    latent_probe: Optional[int]\n",
    "    def __init__(self, input_dim=4, hidden_dim=64, output_dim=1, k=16, aggr='add', norm=None, \n",
    "            loop=True, pool='max',\n",
    "            agg_layers=2, mp_layers=2, in_layers=1, out_layers=3,\n",
    "            graph_features = 0,\n",
    "            latent_probe=None):\n",
    "        super(DynamicReductionNetworkJit, self).__init__()\n",
    "\n",
    "        self.graph_features = graph_features\n",
    "\n",
    "        if latent_probe is not None and (latent_probe>agg_layers+1 or latent_probe<-1*agg_layers-1):\n",
    "            print(\"Error: asked for invalid latent_probe layer\")\n",
    "            return\n",
    "        \n",
    "        if latent_probe is not None and latent_probe < 0:\n",
    "            latent_probe = agg_layers+1 - latent_probe\n",
    "\n",
    "        if latent_probe is not None:\n",
    "            print(\"Probing latent features after %dth layer\"%latent_probe)\n",
    "\n",
    "        self.latent_probe = latent_probe\n",
    "\n",
    "        self.loop = loop\n",
    "\n",
    "        print(\"Pooling with\",pool)\n",
    "        print(\"Using self-loops\" if self.loop else \"Not using self-loops\")\n",
    "        print(\"There are\",agg_layers,'aggregation layers')\n",
    "\n",
    "        if norm is None:\n",
    "            norm = torch.ones(input_dim)\n",
    "\n",
    "        #normalization vector\n",
    "        self.datanorm = nn.Parameter(norm)\n",
    "        \n",
    "        self.k = k\n",
    "\n",
    "        #construct inputnet\n",
    "        in_layers_l = []\n",
    "        in_layers_l += [nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ELU()]\n",
    "\n",
    "        for i in range(in_layers-1):\n",
    "            in_layers_l += [nn.Linear(hidden_dim, hidden_dim), \n",
    "                    nn.ELU()]\n",
    "\n",
    "        self.inputnet = nn.Sequential(*in_layers_l)\n",
    "\n",
    "\n",
    "        #construct aggregation layers\n",
    "        self.agg_layers = nn.ModuleList()\n",
    "        for i in range(agg_layers):\n",
    "            #construct message passing network\n",
    "            mp_layers_l = []\n",
    "\n",
    "            for j in range(mp_layers-1):\n",
    "                mp_layers_l += [nn.Linear(2*hidden_dim, 2*hidden_dim),\n",
    "                        nn.ELU()]\n",
    "\n",
    "            mp_layers_l += [nn.Linear(2*hidden_dim, hidden_dim),\n",
    "                    nn.ELU()]\n",
    "           \n",
    "            convnn = nn.Sequential(*mp_layers_l)\n",
    "            \n",
    "            self.agg_layers.append(EdgeConv(nn=convnn, aggr=aggr).jittable())\n",
    "\n",
    "        #construct outputnet\n",
    "        out_layers_l = []\n",
    "\n",
    "        for i in range(out_layers-1):\n",
    "            out_layers_l += [nn.Linear(hidden_dim+self.graph_features, hidden_dim+self.graph_features), \n",
    "                    nn.ELU()]\n",
    "\n",
    "        out_layers_l += [nn.Linear(hidden_dim+self.graph_features, output_dim)]\n",
    "\n",
    "        self.output = nn.Sequential(*out_layers_l)\n",
    "\n",
    "        if pool not in {'max', 'mean', 'add'}:\n",
    "            raise Exception(\"ERROR: INVALID POOLING\")\n",
    "        \n",
    "        self.aggr_type = pool\n",
    "\n",
    "    def forward(self, x: Tensor, batch: OptTensor, graph_x: OptTensor) -> Tensor:\n",
    "        '''\n",
    "        Push the batch 'data' through the network\n",
    "        '''\n",
    "        x = self.datanorm * x\n",
    "        x = self.inputnet(x)\n",
    "\n",
    "        latent_probe = self.latent_probe\n",
    "        \n",
    "        if graph_x is not None:\n",
    "            graph_x = graph_x.view((-1, self.graph_features))\n",
    "\n",
    "        # if there are no aggregation layers just leave x, batch alone\n",
    "        nAgg = len(self.agg_layers)\n",
    "        for i, edgeconv in enumerate(self.agg_layers):\n",
    "            if latent_probe is not None and i == latent_probe:\n",
    "                return x\n",
    "            knn = knn_graph(x, self.k, batch, loop=self.loop, flow=edgeconv.flow)\n",
    "            edge_index = to_undirected(knn)\n",
    "            x = edgeconv(x, edge_index)\n",
    "\n",
    "            weight = normalized_cut_2d(edge_index, x)\n",
    "            cluster = graclus_cluster(edge_index[0], edge_index[1], weight, x.size(0))\n",
    "\n",
    "            if i == nAgg - 1:\n",
    "                x, batch = aggr_pool_x(cluster, x, batch, self.aggr_type)\n",
    "            else:\n",
    "                x, batch = aggr_pool(cluster, x, batch, self.aggr_type)\n",
    "\n",
    "        if latent_probe is not None and latent_probe == nAgg:\n",
    "            return x\n",
    "\n",
    "        # this xforms to batch-per-row so no need to return batch\n",
    "        x = global_pool_aggr(x, batch, self.aggr_type)\n",
    "\n",
    "        if latent_probe is not None and latent_probe == nAgg + 1:\n",
    "            return x\n",
    "\n",
    "        if graph_x is not None:\n",
    "            x = torch.cat((x, graph_x), 1)\n",
    "\n",
    "        x = self.output(x).squeeze(-1)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f61ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'awkward.highlevel.Array'>\n",
      "[{x: [-1.94, 0, ..., -4.86, -3.89], y: [1.12, ...], z: [...], E: ..., ...}, ...]\n"
     ]
    }
   ],
   "source": [
    "with open('events.pkl', 'rb') as f:\n",
    "    data = pk.load(f)\n",
    "\n",
    "# Now 'data' contains the loaded object\n",
    "print(type(data))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08bcbb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>[{x: [-1.94, 0, ..., -4.86, -3.89], y: [1.12, ..., 5.61], z: [...], ...},\n",
       " {x: [-0.972, -0.972, ..., 1.94, 0.972], y: [2.81, ...], z: [...], E: ..., ...},\n",
       " {x: [-1.94, -2.92, ..., -3.89, -3.89], y: [2.24, ...], z: [...], E: ..., ...},\n",
       " {x: [-0.972, -1.94, ..., -1.94, 0.972], y: [1.68, ...], z: [...], E: ..., ...},\n",
       " {x: [-0.972, -1.94, ..., 0.972, 0.972], y: [0.561, ...], z: [...], ...},\n",
       " {x: [-1.94, -1.94, ..., 0.972, 5.83], y: [1.12, ...], z: [...], E: [...]},\n",
       " {x: [-1.94, -2.92, ..., 2.92, 0.972], y: [2.24, ...], z: [...], E: [...]},\n",
       " {x: [-1.94, 5.83, ..., -1.94, -2.92], y: [2.24, ...], z: [...], E: [...]},\n",
       " {x: [-0.972, -1.94, ..., 0, 0.972], y: [1.68, ..., 3.93], z: [...], ...},\n",
       " {x: [-1.94, -2.92, ..., -2.92, -3.89], y: [1.12, ...], z: [...], E: ..., ...},\n",
       " ...,\n",
       " {x: [-1.94, -1.94, ..., -1.94, -3.89], y: [2.24, ...], z: [...], E: ..., ...},\n",
       " {x: [-1.94, 0.972, ..., -2.92, 0.972], y: [2.24, ...], z: [...], E: ..., ...},\n",
       " {x: [-0.972, -0.972, ..., 0.972], y: [5.05, ...], z: [...], E: [...]},\n",
       " {x: [-0.972, -1.94, ..., -1.94, 0.972], y: [1.68, ...], z: [...], E: ..., ...},\n",
       " {x: [-1.94, -2.92, ..., -3.89, -4.86], y: [1.12, ...], z: [...], E: ..., ...},\n",
       " {x: [-1.94, -2.92, ..., -1.94, -3.89], y: [1.12, ...], z: [...], E: ..., ...},\n",
       " {x: [0, 0, -0.972, ..., 1.94, 0.972], y: [2.24, ...], z: [...], E: [...]},\n",
       " {x: [-1.94, 0, ..., -3.89, -4.86], y: [1.12, ..., 1.68], z: [...], ...},\n",
       " {x: [0, 0, -0.972, ..., -4.86, 0.972], y: [0, ..., 0.561], z: [...], ...}]\n",
       "--------------------------------------------------------------------------------\n",
       "type: 648277 * {\n",
       "    x: var * float64,\n",
       "    y: var * float64,\n",
       "    z: var * float64,\n",
       "    E: var * float64\n",
       "}</pre>"
      ],
      "text/plain": [
       "<Array [{x: [-1.94, ...], y: [...], ...}, ...] type='648277 * {x: var * flo...'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9061a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device name: Tesla T4\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "# Check availability\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# If available, print GPU name\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Device count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a946c715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used RAM: 74.10 GB\n",
      "Total RAM: 173.08 GB\n",
      "Available RAM: 97.34 GB\n"
     ]
    }
   ],
   "source": [
    "# RAM used in GB\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Used RAM: {mem.used / 1e9:.2f} GB\")\n",
    "print(f\"Total RAM: {mem.total / 1e9:.2f} GB\")\n",
    "print(f\"Available RAM: {mem.available / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc2c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torchify(feat, graph_x = None):\n",
    "    data = [Data(x = torch.from_numpy(ak.to_numpy(ele).astype(np.float32))) for ele in feat]\n",
    "    if graph_x is not None:\n",
    "        for d, gx in zip(data, graph_x):\n",
    "            d.graph_x = gx\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68478d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torchify_drns(ak_array):\n",
    "    \"\"\"\n",
    "    Convert Awkward array of variable-length [x, y, z, E] hits \n",
    "    into list of PyTorch Geometric Data objects.\n",
    "    \"\"\"\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for event in ak_array:\n",
    "        # Extract individual arrays\n",
    "        x = np.asarray(event[\"x\"], dtype=np.float32)\n",
    "        y = np.asarray(event[\"y\"], dtype=np.float32)\n",
    "        z = np.asarray(event[\"z\"], dtype=np.float32)\n",
    "        E = np.asarray(event[\"E\"], dtype=np.float32)\n",
    "\n",
    "        # Stack to shape: (num_hits, 4)\n",
    "        features = np.stack([x, y, z, E], axis=1)  # shape: [num_hits, 4]\n",
    "\n",
    "        # Convert to tensor\n",
    "        x_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "        # Create Data object\n",
    "        data = Data(x=x_tensor)\n",
    "\n",
    "        data_list.append(data)\n",
    "\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2896bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchified_data = torchify_drns(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a387a731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchified_data[0].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6ea122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"hgcal_electron_data_0001.h5\", \"r\") as f:\n",
    "    target_energy = f[\"target\"][:]  # shape: (num_events,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d641def",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(target_energy, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2be003cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(torchified_data):\n",
    "    data.y = y[i].unsqueeze(0)  # shape: (1,) to match output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2221bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8beb8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torchified_data, \"torchified_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cfbf90f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "648277"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torchified_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e57aeb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torchified_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "044e1373",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dac37fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(torchified_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "518965ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooling with mean\n",
      "Using self-loops\n",
      "There are 2 aggregation layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_107_cuda/x86_64-el9-gcc11-opt/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:1032: UserWarning: 'EdgeConv.jittable' is deprecated and a no-op. Please remove its usage.\n",
      "  warnings.warn(f\"'{self.__class__.__name__}.jittable' is deprecated \"\n"
     ]
    }
   ],
   "source": [
    "model = DynamicReductionNetworkJit(\n",
    "    input_dim=4,        # [x, y, z, E]\n",
    "    hidden_dim=64,\n",
    "    output_dim=1,       # regression target\n",
    "    k=8,                # or 16 or 24\n",
    "    aggr='add',         # 'mean' or 'max' also possible\n",
    "    pool='mean',        # or 'max', 'add'\n",
    "    agg_layers=2,\n",
    "    mp_layers=2,\n",
    "    in_layers=1,\n",
    "    out_layers=2,\n",
    "    graph_features=0,   # set >0 if graph_x is used\n",
    "    latent_probe=None   # or an int to return intermediate latent\n",
    ")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b770ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()  # or MAE, SmoothL1Loss, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(1, 51):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.batch, batch.graph_x if hasattr(batch, 'graph_x') else None)\n",
    "        loss = loss_fn(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch:02d}, Loss: {total_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
